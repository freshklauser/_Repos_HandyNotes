{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Wang\\\\Anaconda3\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MappingProxyType',\n",
       " 'RLock',\n",
       " 'WRAPPER_ASSIGNMENTS',\n",
       " 'WRAPPER_UPDATES',\n",
       " 'WeakKeyDictionary',\n",
       " 'cmp_to_key',\n",
       " 'get_cache_token',\n",
       " 'lru_cache',\n",
       " 'namedtuple',\n",
       " 'partial',\n",
       " 'partialmethod',\n",
       " 'recursive_repr',\n",
       " 'reduce',\n",
       " 'singledispatch',\n",
       " 'total_ordering',\n",
       " 'update_wrapper',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functools\n",
    "import functools\n",
    "funcs = [ele for ele in dir(functools) if not ele.startswith(\"_\")]\n",
    "funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class partial in module functools:\n",
      "\n",
      "class partial(builtins.object)\n",
      " |  partial(func, *args, **keywords) - new function with partial application\n",
      " |  of the given arguments and keywords.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, /, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name, /)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value, /)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |  \n",
      " |  args\n",
      " |      tuple of arguments to future partial calls\n",
      " |  \n",
      " |  func\n",
      " |      function object to use in future partial calls\n",
      " |  \n",
      " |  keywords\n",
      " |      dictionary of keyword arguments to future partial calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(functools.partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['append',\n",
       " 'appendleft',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'extendleft',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'maxlen',\n",
       " 'pop',\n",
       " 'popleft',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'rotate']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "[ele for ele in dir(collections.deque) if not ele.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package steppy:\n",
      "\n",
      "NAME\n",
      "    steppy\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    adapter\n",
      "    base\n",
      "    utils\n",
      "\n",
      "FILE\n",
      "    c:\\users\\wang\\anaconda3\\lib\\site-packages\\steppy\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import steppy\n",
    "help(steppy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module steppy.adapter in steppy:\n",
      "\n",
      "NAME\n",
      "    steppy.adapter\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        AdapterError\n",
      "    builtins.object\n",
      "        Adapter\n",
      "    builtins.tuple(builtins.object)\n",
      "        E\n",
      "    \n",
      "    class Adapter(builtins.object)\n",
      "     |  Translates outputs from parent steps to inputs to the current step.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      adapting_recipes: The recipes that the adapter was initialized with.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      Normally Adapter is used with a Step. In the following example\n",
      "     |      `RandomForestTransformer` follows sklearn convention of calling arguments `X` and `y`,\n",
      "     |      however names passed to the Step are different. We use Adapter to map recieved names\n",
      "     |      to the expected names.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from sklearn.datasets import load_iris\n",
      "     |          from sklearn.ensemble import RandomForestClassifier\n",
      "     |          from sklearn.metrics import log_loss\n",
      "     |          from steppy.base import BaseTransformer, Step\n",
      "     |          from steppy.adapter import Adapter, E\n",
      "     |  \n",
      "     |          iris = load_iris()\n",
      "     |  \n",
      "     |          pipeline_input = {\n",
      "     |              'train_data': {\n",
      "     |                  'target': iris.target,\n",
      "     |                  'data': iris.data\n",
      "     |              }\n",
      "     |          }\n",
      "     |  \n",
      "     |          class RandomForestTransformer(BaseTransformer):\n",
      "     |              def __init__(self, random_state=None):\n",
      "     |                  self.estimator = RandomForestClassifier(random_state=random_state)\n",
      "     |  \n",
      "     |              def fit(self, X, y):\n",
      "     |                  self.estimator.fit(X, y)\n",
      "     |                  return self\n",
      "     |  \n",
      "     |              def transform(self, X, **kwargs):\n",
      "     |                  y_proba  = self.estimator.predict_proba(X)\n",
      "     |                  return {'y_proba': y_proba}\n",
      "     |  \n",
      "     |          random_forest = Step(\n",
      "     |              name=\"random_forest\",\n",
      "     |              transformer=RandomForestTransformer(),\n",
      "     |              input_data=['train_data'],\n",
      "     |              adapter=Adapter({\n",
      "     |                  'X': E('train_data', 'data'),\n",
      "     |                  'y': E('train_data', 'target')\n",
      "     |              }),\n",
      "     |              experiment_directory='./working_dir'\n",
      "     |          )\n",
      "     |  \n",
      "     |          result = random_forest.fit_transform(pipeline_input)\n",
      "     |          print(log_loss(y_true=iris.target, y_pred=result['y_proba']))\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, adapting_recipes:Dict[str, Any])\n",
      "     |      Adapter constructor.\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          You have to import the extractor 'E' from this module to construct\n",
      "     |          adapters.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          adapting_recipes: Recipes used to control the input translation.\n",
      "     |              An adapting recipe may be any Python data structure. If this structure\n",
      "     |              contains placeholders denoted by `E`, then values extracted from parent\n",
      "     |              steps' outputs will be substituted in their place.\n",
      "     |              `adapting_recipes` is a dict where the keys match the arguments\n",
      "     |              expected by the transformer. The values in this dictionary may be for example\n",
      "     |              one of the following:\n",
      "     |      \n",
      "     |              1. `E('input_name', 'key')` will query the parent step\n",
      "     |                  'input_name' for the output 'key'\n",
      "     |      \n",
      "     |              2. List of `E('input_name', 'key')` will apply the extractors\n",
      "     |                  to the parent steps and combine the results into a list\n",
      "     |      \n",
      "     |              3. Tuple of `E('input_name', 'key')` will apply the extractors\n",
      "     |                  to the parent steps and combine the results into a tuple\n",
      "     |      \n",
      "     |              4. Dict like `{k: E('input_name', 'key')}` will apply the\n",
      "     |                  extractors to the parent steps and combine the results\n",
      "     |                  into a dict with the same keys\n",
      "     |      \n",
      "     |              5. Anything else: the value itself will be used as the argument\n",
      "     |                  to the transformer\n",
      "     |  \n",
      "     |  adapt(self, all_ouputs:Dict[str, Dict[str, Any]]) -> Dict[str, Any]\n",
      "     |      Adapt inputs for the transformer included in the step.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          all_ouputs: Dict of outputs from parent steps. The keys should\n",
      "     |              match the names of these steps and the values should be their\n",
      "     |              respective outputs.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Dictionary with the same keys as `adapting_recipes` and values\n",
      "     |          constructed according to the respective recipes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class AdapterError(builtins.Exception)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdapterError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class E(builtins.tuple)\n",
      "     |  E(input_name, key)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      E\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new OrderedDict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(_self, **kwds)\n",
      "     |      Return a new E object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  _make(iterable, new=<built-in method __new__ of type object at 0x000000006AE7B8E0>, len=<built-in function len>) from builtins.type\n",
      "     |      Make a new E object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(_cls, input_name, key)\n",
      "     |      Create new instance of E(input_name, key)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  input_name\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  key\n",
      "     |      Alias for field number 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = OrderedDict([('input_name', <class 'str'>), ('key', ...\n",
      "     |  \n",
      "     |  _field_types = OrderedDict([('input_name', <class 'str'>), ('key', <cl...\n",
      "     |  \n",
      "     |  _fields = ('input_name', 'key')\n",
      "     |  \n",
      "     |  _source = \"from builtins import property as _property, tupl..._itemget...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(...)\n",
      "     |      T.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  index(...)\n",
      "     |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "\n",
      "DATA\n",
      "    AdaptingRecipe = typing.Any\n",
      "    Any = typing.Any\n",
      "\n",
      "FILE\n",
      "    c:\\users\\wang\\anaconda3\\lib\\site-packages\\steppy\\adapter.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(steppy.adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'type'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['all_upstream_steps',\n",
       " 'clean_cache_step',\n",
       " 'clean_cache_upstream',\n",
       " 'experiment_directory_output_step',\n",
       " 'experiment_directory_transformers_step',\n",
       " 'fit_transform',\n",
       " 'get_step_by_name',\n",
       " 'output_is_cached',\n",
       " 'output_is_persisted',\n",
       " 'persist_upstream_diagram',\n",
       " 'persist_upstream_structure',\n",
       " 'reset',\n",
       " 'set_mode_inference',\n",
       " 'set_mode_train',\n",
       " 'set_parameters_upstream',\n",
       " 'transform',\n",
       " 'transformer_is_persisted',\n",
       " 'upstream_structure']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from steppy.base import Step   # Step is a building block of steppy pipelines\n",
    "[ele for ele in dir(Step) if not ele.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _isotonic\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, safe=True)\n",
      "        Constructs a new estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It yields a new estimator\n",
      "        with the same parameters that has not been fit on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object, or list, tuple or set of objects\n",
      "            The estimator or group of estimators to be cloned\n",
      "        \n",
      "        safe : boolean, optional\n",
      "            If safe is false, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "    \n",
      "    config_context(**new_config)\n",
      "        Context manager for global scikit-learn configuration\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, optional\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited. This is not\n",
      "        thread-safe.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN, ...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, optional\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, optional\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    0.21.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\wang\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "[ele for ele in dir(sklearn) if not ele.startswith('_')]\n",
    "print(help(sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package joblib:\n",
      "\n",
      "NAME\n",
      "    joblib\n",
      "\n",
      "DESCRIPTION\n",
      "    Joblib is a set of tools to provide **lightweight pipelining in\n",
      "    Python**. In particular:\n",
      "    \n",
      "    1. transparent disk-caching of functions and lazy re-evaluation\n",
      "       (memoize pattern)\n",
      "    \n",
      "    2. easy simple parallel computing\n",
      "    \n",
      "    Joblib is optimized to be **fast** and **robust** in particular on large\n",
      "    data and has specific optimizations for `numpy` arrays. It is\n",
      "    **BSD-licensed**.\n",
      "    \n",
      "    \n",
      "        ==================== ===============================================\n",
      "        **Documentation:**       https://joblib.readthedocs.io\n",
      "    \n",
      "        **Download:**            https://pypi.python.org/pypi/joblib#downloads\n",
      "    \n",
      "        **Source code:**         https://github.com/joblib/joblib\n",
      "    \n",
      "        **Report issues:**       https://github.com/joblib/joblib/issues\n",
      "        ==================== ===============================================\n",
      "    \n",
      "    \n",
      "    Vision\n",
      "    --------\n",
      "    \n",
      "    The vision is to provide tools to easily achieve better performance and\n",
      "    reproducibility when working with long running jobs.\n",
      "    \n",
      "     *  **Avoid computing twice the same thing**: code is rerun over an\n",
      "        over, for instance when prototyping computational-heavy jobs (as in\n",
      "        scientific development), but hand-crafted solution to alleviate this\n",
      "        issue is error-prone and often leads to unreproducible results\n",
      "    \n",
      "     *  **Persist to disk transparently**: persisting in an efficient way\n",
      "        arbitrary objects containing large data is hard. Using\n",
      "        joblib's caching mechanism avoids hand-written persistence and\n",
      "        implicitly links the file on disk to the execution context of\n",
      "        the original Python object. As a result, joblib's persistence is\n",
      "        good for resuming an application status or computational job, eg\n",
      "        after a crash.\n",
      "    \n",
      "    Joblib addresses these problems while **leaving your code and your flow\n",
      "    control as unmodified as possible** (no framework, no new paradigms).\n",
      "    \n",
      "    Main features\n",
      "    ------------------\n",
      "    \n",
      "    1) **Transparent and fast disk-caching of output value:** a memoize or\n",
      "       make-like functionality for Python functions that works well for\n",
      "       arbitrary Python objects, including very large numpy arrays. Separate\n",
      "       persistence and flow-execution logic from domain logic or algorithmic\n",
      "       code by writing the operations as a set of steps with well-defined\n",
      "       inputs and  outputs: Python functions. Joblib can save their\n",
      "       computation to disk and rerun it only if necessary::\n",
      "    \n",
      "          >>> from joblib import Memory\n",
      "          >>> cachedir = 'your_cache_dir_goes_here'\n",
      "          >>> mem = Memory(cachedir)\n",
      "          >>> import numpy as np\n",
      "          >>> a = np.vander(np.arange(3)).astype(np.float)\n",
      "          >>> square = mem.cache(np.square)\n",
      "          >>> b = square(a)                                   # doctest: +ELLIPSIS\n",
      "          ________________________________________________________________________________\n",
      "          [Memory] Calling square...\n",
      "          square(array([[0., 0., 1.],\n",
      "                 [1., 1., 1.],\n",
      "                 [4., 2., 1.]]))\n",
      "          ___________________________________________________________square - 0...s, 0.0min\n",
      "    \n",
      "          >>> c = square(a)\n",
      "          >>> # The above call did not trigger an evaluation\n",
      "    \n",
      "    2) **Embarrassingly parallel helper:** to make it easy to write readable\n",
      "       parallel code and debug it quickly::\n",
      "    \n",
      "          >>> from joblib import Parallel, delayed\n",
      "          >>> from math import sqrt\n",
      "          >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n",
      "          [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "    \n",
      "    \n",
      "    3) **Fast compressed Persistence**: a replacement for pickle to work\n",
      "       efficiently on Python objects containing large data (\n",
      "       *joblib.dump* & *joblib.load* ).\n",
      "    \n",
      "    ..\n",
      "        >>> import shutil ; shutil.rmtree(cachedir)\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _compat\n",
      "    _dask\n",
      "    _memmapping_reducer\n",
      "    _memory_helpers\n",
      "    _multiprocessing_helpers\n",
      "    _parallel_backends\n",
      "    _store_backends\n",
      "    backports\n",
      "    compressor\n",
      "    disk\n",
      "    executor\n",
      "    externals (package)\n",
      "    format_stack\n",
      "    func_inspect\n",
      "    hashing\n",
      "    logger\n",
      "    memory\n",
      "    my_exceptions\n",
      "    numpy_pickle\n",
      "    numpy_pickle_compat\n",
      "    numpy_pickle_utils\n",
      "    parallel\n",
      "    pool\n",
      "    test (package)\n",
      "    testing\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        joblib.logger.Logger\n",
      "            joblib.memory.MemorizedResult\n",
      "            joblib.memory.Memory\n",
      "            joblib.parallel.Parallel\n",
      "        joblib.logger.PrintTime\n",
      "        joblib.parallel.parallel_backend\n",
      "    \n",
      "    class Logger(builtins.object)\n",
      "     |  Base class for logging messages.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, depth=3)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MemorizedResult(joblib.logger.Logger)\n",
      "     |  Object representing a cached value.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  location: str\n",
      "     |      The location of joblib cache. Depends on the store backend used.\n",
      "     |  \n",
      "     |  func: function or str\n",
      "     |      function whose output is cached. The string case is intended only for\n",
      "     |      instanciation based on the output of repr() on another instance.\n",
      "     |      (namely eval(repr(memorized_instance)) works).\n",
      "     |  \n",
      "     |  argument_hash: str\n",
      "     |      hash of the function arguments.\n",
      "     |  \n",
      "     |  backend: str\n",
      "     |      Type of store backend for reading/writing cache files.\n",
      "     |      Default is 'local'.\n",
      "     |  \n",
      "     |  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n",
      "     |      The memmapping mode used when loading from cache numpy arrays. See\n",
      "     |      numpy.load for the meaning of the different values.\n",
      "     |  \n",
      "     |  verbose: int\n",
      "     |      verbosity level (0 means no message).\n",
      "     |  \n",
      "     |  timestamp, metadata: string\n",
      "     |      for internal use only.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MemorizedResult\n",
      "     |      joblib.logger.Logger\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, location, func, args_id, backend='local', mmap_mode=None, verbose=0, timestamp=None, metadata=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      Clear value from cache\n",
      "     |  \n",
      "     |  get(self)\n",
      "     |      Read value from cache and return it.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  argument_hash\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Memory(joblib.logger.Logger)\n",
      "     |  A context object for caching a function's return value each time it\n",
      "     |  is called with the same input arguments.\n",
      "     |  \n",
      "     |  All values are cached on the filesystem, in a deep directory\n",
      "     |  structure.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <memory>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  location: str or None\n",
      "     |      The path of the base directory to use as a data store\n",
      "     |      or None. If None is given, no caching is done and\n",
      "     |      the Memory object is completely transparent. This option\n",
      "     |      replaces cachedir since version 0.12.\n",
      "     |  \n",
      "     |  backend: str, optional\n",
      "     |      Type of store backend for reading/writing cache files.\n",
      "     |      Default: 'local'.\n",
      "     |      The 'local' backend is using regular filesystem operations to\n",
      "     |      manipulate data (open, mv, etc) in the backend.\n",
      "     |  \n",
      "     |  cachedir: str or None, optional\n",
      "     |  \n",
      "     |      .. deprecated: 0.12\n",
      "     |          'cachedir' has been deprecated in 0.12 and will be\n",
      "     |          removed in 0.14. Use the 'location' parameter instead.\n",
      "     |  \n",
      "     |  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n",
      "     |      The memmapping mode used when loading from cache\n",
      "     |      numpy arrays. See numpy.load for the meaning of the\n",
      "     |      arguments.\n",
      "     |  \n",
      "     |  compress: boolean, or integer, optional\n",
      "     |      Whether to zip the stored data on disk. If an integer is\n",
      "     |      given, it should be between 1 and 9, and sets the amount\n",
      "     |      of compression. Note that compressed arrays cannot be\n",
      "     |      read by memmapping.\n",
      "     |  \n",
      "     |  verbose: int, optional\n",
      "     |      Verbosity flag, controls the debug messages that are issued\n",
      "     |      as functions are evaluated.\n",
      "     |  \n",
      "     |  bytes_limit: int, optional\n",
      "     |      Limit in bytes of the size of the cache.\n",
      "     |  \n",
      "     |  backend_options: dict, optional\n",
      "     |      Contains a dictionnary of named parameters used to configure\n",
      "     |      the store backend.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Memory\n",
      "     |      joblib.logger.Logger\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      We don't store the timestamp when pickling, to avoid the hash\n",
      "     |      depending from it.\n",
      "     |  \n",
      "     |  __init__(self, location=None, backend='local', cachedir=None, mmap_mode=None, compress=False, verbose=1, bytes_limit=None, backend_options=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cache(self, func=None, ignore=None, verbose=None, mmap_mode=False)\n",
      "     |      Decorates the given function func to only compute its return\n",
      "     |      value for input arguments not cached on disk.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func: callable, optional\n",
      "     |          The function to be decorated\n",
      "     |      ignore: list of strings\n",
      "     |          A list of arguments name to ignore in the hashing\n",
      "     |      verbose: integer, optional\n",
      "     |          The verbosity mode of the function. By default that\n",
      "     |          of the memory object is used.\n",
      "     |      mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n",
      "     |          The memmapping mode used when loading from cache\n",
      "     |          numpy arrays. See numpy.load for the meaning of the\n",
      "     |          arguments. By default that of the memory object is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decorated_func: MemorizedFunc object\n",
      "     |          The returned object is a MemorizedFunc object, that is\n",
      "     |          callable (behaves like a function), but offers extra\n",
      "     |          methods for cache lookup and management. See the\n",
      "     |          documentation for :class:`joblib.memory.MemorizedFunc`.\n",
      "     |  \n",
      "     |  clear(self, warn=True)\n",
      "     |      Erase the complete cache directory.\n",
      "     |  \n",
      "     |  eval(self, func, *args, **kwargs)\n",
      "     |      Eval function func with arguments `*args` and `**kwargs`,\n",
      "     |      in the context of the memory.\n",
      "     |      \n",
      "     |      This method works similarly to the builtin `apply`, except\n",
      "     |      that the function is called only if the cache is not\n",
      "     |      up to date.\n",
      "     |  \n",
      "     |  reduce_size(self)\n",
      "     |      Remove cache elements to make cache size fit in ``bytes_limit``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  cachedir\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Parallel(joblib.logger.Logger)\n",
      "     |  Helper class for readable parallel mapping.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <parallel>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  -----------\n",
      "     |  n_jobs: int, default: None\n",
      "     |      The maximum number of concurrently running jobs, such as the number\n",
      "     |      of Python worker processes when backend=\"multiprocessing\"\n",
      "     |      or the size of the thread-pool when backend=\"threading\".\n",
      "     |      If -1 all CPUs are used. If 1 is given, no parallel computing code\n",
      "     |      is used at all, which is useful for debugging. For n_jobs below -1,\n",
      "     |      (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n",
      "     |      CPUs but one are used.\n",
      "     |      None is a marker for 'unset' that will be interpreted as n_jobs=1\n",
      "     |      (sequential execution) unless the call is performed under a\n",
      "     |      parallel_backend context manager that sets another value for\n",
      "     |      n_jobs.\n",
      "     |  backend: str, ParallelBackendBase instance or None, default: 'loky'\n",
      "     |      Specify the parallelization backend implementation.\n",
      "     |      Supported backends are:\n",
      "     |  \n",
      "     |      - \"loky\" used by default, can induce some\n",
      "     |        communication and memory overhead when exchanging input and\n",
      "     |        output data with the worker Python processes.\n",
      "     |      - \"multiprocessing\" previous process-based backend based on\n",
      "     |        `multiprocessing.Pool`. Less robust than `loky`.\n",
      "     |      - \"threading\" is a very low-overhead backend but it suffers\n",
      "     |        from the Python Global Interpreter Lock if the called function\n",
      "     |        relies a lot on Python objects. \"threading\" is mostly useful\n",
      "     |        when the execution bottleneck is a compiled extension that\n",
      "     |        explicitly releases the GIL (for instance a Cython loop wrapped\n",
      "     |        in a \"with nogil\" block or an expensive call to a library such\n",
      "     |        as NumPy).\n",
      "     |      - finally, you can register backends by calling\n",
      "     |        register_parallel_backend. This will allow you to implement\n",
      "     |        a backend of your liking.\n",
      "     |  \n",
      "     |      It is not recommended to hard-code the backend name in a call to\n",
      "     |      Parallel in a library. Instead it is recommended to set soft hints\n",
      "     |      (prefer) or hard constraints (require) so as to make it possible\n",
      "     |      for library users to change the backend from the outside using the\n",
      "     |      parallel_backend context manager.\n",
      "     |  prefer: str in {'processes', 'threads'} or None, default: None\n",
      "     |      Soft hint to choose the default backend if no specific backend\n",
      "     |      was selected with the parallel_backend context manager. The\n",
      "     |      default process-based backend is 'loky' and the default\n",
      "     |      thread-based backend is 'threading'.\n",
      "     |  require: 'sharedmem' or None, default None\n",
      "     |      Hard constraint to select the backend. If set to 'sharedmem',\n",
      "     |      the selected backend will be single-host and thread-based even\n",
      "     |      if the user asked for a non-thread based backend with\n",
      "     |      parallel_backend.\n",
      "     |  verbose: int, optional\n",
      "     |      The verbosity level: if non zero, progress messages are\n",
      "     |      printed. Above 50, the output is sent to stdout.\n",
      "     |      The frequency of the messages increases with the verbosity level.\n",
      "     |      If it more than 10, all iterations are reported.\n",
      "     |  timeout: float, optional\n",
      "     |      Timeout limit for each task to complete.  If any task takes longer\n",
      "     |      a TimeOutError will be raised. Only applied when n_jobs != 1\n",
      "     |  pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n",
      "     |      The number of batches (of tasks) to be pre-dispatched.\n",
      "     |      Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n",
      "     |      default and the workers should never starve.\n",
      "     |  batch_size: int or 'auto', default: 'auto'\n",
      "     |      The number of atomic tasks to dispatch at once to each\n",
      "     |      worker. When individual evaluations are very fast, dispatching\n",
      "     |      calls to workers can be slower than sequential computation because\n",
      "     |      of the overhead. Batching fast computations together can mitigate\n",
      "     |      this.\n",
      "     |      The ``'auto'`` strategy keeps track of the time it takes for a batch\n",
      "     |      to complete, and dynamically adjusts the batch size to keep the time\n",
      "     |      on the order of half a second, using a heuristic. The initial batch\n",
      "     |      size is 1.\n",
      "     |      ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n",
      "     |      batches of a single task at a time as the threading backend has\n",
      "     |      very little overhead and using larger batch size has not proved to\n",
      "     |      bring any gain in that case.\n",
      "     |  temp_folder: str, optional\n",
      "     |      Folder to be used by the pool for memmapping large arrays\n",
      "     |      for sharing memory with worker processes. If None, this will try in\n",
      "     |      order:\n",
      "     |  \n",
      "     |      - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n",
      "     |        variable,\n",
      "     |      - /dev/shm if the folder exists and is writable: this is a\n",
      "     |        RAM disk filesystem available by default on modern Linux\n",
      "     |        distributions,\n",
      "     |      - the default system temporary folder that can be\n",
      "     |        overridden with TMP, TMPDIR or TEMP environment\n",
      "     |        variables, typically /tmp under Unix operating systems.\n",
      "     |  \n",
      "     |      Only active when backend=\"loky\" or \"multiprocessing\".\n",
      "     |  max_nbytes int, str, or None, optional, 1M by default\n",
      "     |      Threshold on the size of arrays passed to the workers that\n",
      "     |      triggers automated memory mapping in temp_folder. Can be an int\n",
      "     |      in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n",
      "     |      Use None to disable memmapping of large arrays.\n",
      "     |      Only active when backend=\"loky\" or \"multiprocessing\".\n",
      "     |  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n",
      "     |      Memmapping mode for numpy arrays passed to workers.\n",
      "     |      See 'max_nbytes' parameter documentation for more details.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  This object uses workers to compute in parallel the application of a\n",
      "     |  function to many different arguments. The main functionality it brings\n",
      "     |  in addition to using the raw multiprocessing or concurrent.futures API\n",
      "     |  are (see examples for details):\n",
      "     |  \n",
      "     |  * More readable code, in particular since it avoids\n",
      "     |    constructing list of arguments.\n",
      "     |  \n",
      "     |  * Easier debugging:\n",
      "     |      - informative tracebacks even when the error happens on\n",
      "     |        the client side\n",
      "     |      - using 'n_jobs=1' enables to turn off parallel computing\n",
      "     |        for debugging without changing the codepath\n",
      "     |      - early capture of pickling errors\n",
      "     |  \n",
      "     |  * An optional progress meter.\n",
      "     |  \n",
      "     |  * Interruption of multiprocesses jobs with 'Ctrl-C'\n",
      "     |  \n",
      "     |  * Flexible pickling control for the communication to and from\n",
      "     |    the worker processes.\n",
      "     |  \n",
      "     |  * Ability to use shared memory efficiently with worker\n",
      "     |    processes for large numpy-based datastructures.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  A simple example:\n",
      "     |  \n",
      "     |  >>> from math import sqrt\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n",
      "     |  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "     |  \n",
      "     |  Reshaping the output when the function has several return\n",
      "     |  values:\n",
      "     |  \n",
      "     |  >>> from math import modf\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n",
      "     |  >>> res, i = zip(*r)\n",
      "     |  >>> res\n",
      "     |  (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n",
      "     |  >>> i\n",
      "     |  (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n",
      "     |  \n",
      "     |  The progress meter: the higher the value of `verbose`, the more\n",
      "     |  messages:\n",
      "     |  \n",
      "     |  >>> from time import sleep\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n",
      "     |  [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n",
      "     |  [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n",
      "     |  [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n",
      "     |  \n",
      "     |  Traceback example, note how the line of the error is indicated\n",
      "     |  as well as the values of the parameter passed to the function that\n",
      "     |  triggered the exception, even though the traceback happens in the\n",
      "     |  child process:\n",
      "     |  \n",
      "     |  >>> from heapq import nlargest\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n",
      "     |  #...\n",
      "     |  ---------------------------------------------------------------------------\n",
      "     |  Sub-process traceback:\n",
      "     |  ---------------------------------------------------------------------------\n",
      "     |  TypeError                                          Mon Nov 12 11:37:46 2012\n",
      "     |  PID: 12934                                    Python 2.7.3: /usr/bin/python\n",
      "     |  ...........................................................................\n",
      "     |  /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n",
      "     |      419         if n >= size:\n",
      "     |      420             return sorted(iterable, key=key, reverse=True)[:n]\n",
      "     |      421\n",
      "     |      422     # When key is none, use simpler decoration\n",
      "     |      423     if key is None:\n",
      "     |  --> 424         it = izip(iterable, count(0,-1))                    # decorate\n",
      "     |      425         result = _nlargest(n, it)\n",
      "     |      426         return map(itemgetter(0), result)                   # undecorate\n",
      "     |      427\n",
      "     |      428     # General case, slowest method\n",
      "     |   TypeError: izip argument #1 must support iteration\n",
      "     |  ___________________________________________________________________________\n",
      "     |  \n",
      "     |  \n",
      "     |  Using pre_dispatch in a producer/consumer situation, where the\n",
      "     |  data is generated on the fly. Note how the producer is first\n",
      "     |  called 3 times before the parallel loop is initiated, and then\n",
      "     |  called to generate new data on the fly:\n",
      "     |  \n",
      "     |  >>> from math import sqrt\n",
      "     |  >>> from joblib import Parallel, delayed\n",
      "     |  >>> def producer():\n",
      "     |  ...     for i in range(6):\n",
      "     |  ...         print('Produced %s' % i)\n",
      "     |  ...         yield i\n",
      "     |  >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n",
      "     |  ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n",
      "     |  Produced 0\n",
      "     |  Produced 1\n",
      "     |  Produced 2\n",
      "     |  [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n",
      "     |  Produced 3\n",
      "     |  [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n",
      "     |  Produced 4\n",
      "     |  [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n",
      "     |  Produced 5\n",
      "     |  [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n",
      "     |  [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n",
      "     |  [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Parallel\n",
      "     |      joblib.logger.Logger\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, iterable)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, exc_type, exc_value, traceback)\n",
      "     |  \n",
      "     |  __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=None, max_nbytes='1M', mmap_mode='r', prefer=None, require=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      depth: int, optional\n",
      "     |          The depth of objects printed.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  dispatch_next(self)\n",
      "     |      Dispatch more data for parallel processing\n",
      "     |      \n",
      "     |      This method is meant to be called concurrently by the multiprocessing\n",
      "     |      callback. We rely on the thread-safety of dispatch_one_batch to protect\n",
      "     |      against concurrent consumption of the unprotected iterator.\n",
      "     |  \n",
      "     |  dispatch_one_batch(self, iterator)\n",
      "     |      Prefetch the tasks for the next batch and dispatch them.\n",
      "     |      \n",
      "     |      The effective size of the batch is computed here.\n",
      "     |      If there are no more jobs to dispatch, return False, else return True.\n",
      "     |      \n",
      "     |      The iterator consumption and dispatching is protected by the same\n",
      "     |      lock so calling this function should be thread safe.\n",
      "     |  \n",
      "     |  print_progress(self)\n",
      "     |      Display the process of the parallel execution only a fraction\n",
      "     |      of time, controlled by self.verbose.\n",
      "     |  \n",
      "     |  retrieve(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  debug(self, msg)\n",
      "     |  \n",
      "     |  format(self, obj, indent=0)\n",
      "     |      Return the formatted representation of the object.\n",
      "     |  \n",
      "     |  warn(self, msg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from joblib.logger.Logger:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PrintTime(builtins.object)\n",
      "     |  Print and log messages while keeping track of time.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, msg='', total=False)\n",
      "     |      Print the time elapsed between the last call and the current\n",
      "     |      call, with an optional message.\n",
      "     |  \n",
      "     |  __init__(self, logfile=None, logdir=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class parallel_backend(builtins.object)\n",
      "     |  Change the default backend used by Parallel inside a with block.\n",
      "     |  \n",
      "     |  If ``backend`` is a string it must match a previously registered\n",
      "     |  implementation using the ``register_parallel_backend`` function.\n",
      "     |  \n",
      "     |  By default the following backends are available:\n",
      "     |  \n",
      "     |  - 'loky': single-host, process-based parallelism (used by default),\n",
      "     |  - 'threading': single-host, thread-based parallelism,\n",
      "     |  - 'multiprocessing': legacy single-host, process-based parallelism.\n",
      "     |  \n",
      "     |  'loky' is recommended to run functions that manipulate Python objects.\n",
      "     |  'threading' is a low-overhead alternative that is most efficient for\n",
      "     |  functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n",
      "     |  CPU-bound code in a few calls to native code that explicitly releases the\n",
      "     |  GIL.\n",
      "     |  \n",
      "     |  In addition, if the `dask` and `distributed` Python packages are installed,\n",
      "     |  it is possible to use the 'dask' backend for better scheduling of nested\n",
      "     |  parallel calls without over-subscription and potentially distribute\n",
      "     |  parallel calls over a networked cluster of several hosts.\n",
      "     |  \n",
      "     |  Alternatively the backend can be passed directly as an instance.\n",
      "     |  \n",
      "     |  By default all available workers will be used (``n_jobs=-1``) unless the\n",
      "     |  caller passes an explicit value for the ``n_jobs`` parameter.\n",
      "     |  \n",
      "     |  This is an alternative to passing a ``backend='backend_name'`` argument to\n",
      "     |  the ``Parallel`` class constructor. It is particularly useful when calling\n",
      "     |  into library code that uses joblib internally but does not expose the\n",
      "     |  backend argument in its own API.\n",
      "     |  \n",
      "     |  >>> from operator import neg\n",
      "     |  >>> with parallel_backend('threading'):\n",
      "     |  ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n",
      "     |  ...\n",
      "     |  [-1, -2, -3, -4, -5]\n",
      "     |  \n",
      "     |  Warning: this function is experimental and subject to change in a future\n",
      "     |  version of joblib.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.10\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, type, value, traceback)\n",
      "     |  \n",
      "     |  __init__(self, backend, n_jobs=-1, **backend_params)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  unregister(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    cpu_count()\n",
      "        Return the number of CPUs.\n",
      "    \n",
      "    delayed(function, check_pickle=None)\n",
      "        Decorator used to capture the arguments of a function.\n",
      "    \n",
      "    dump(value, filename, compress=0, protocol=None, cache_size=None)\n",
      "        Persist an arbitrary Python object into one file.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <persistence>`.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        value: any Python object\n",
      "            The object to store to disk.\n",
      "        filename: str, pathlib.Path, or file object.\n",
      "            The file object or path of the file in which it is to be stored.\n",
      "            The compression method corresponding to one of the supported filename\n",
      "            extensions ('.z', '.gz', '.bz2', '.xz' or '.lzma') will be used\n",
      "            automatically.\n",
      "        compress: int from 0 to 9 or bool or 2-tuple, optional\n",
      "            Optional compression level for the data. 0 or False is no compression.\n",
      "            Higher value means more compression, but also slower read and\n",
      "            write times. Using a value of 3 is often a good compromise.\n",
      "            See the notes for more details.\n",
      "            If compress is True, the compression level used is 3.\n",
      "            If compress is a 2-tuple, the first element must correspond to a string\n",
      "            between supported compressors (e.g 'zlib', 'gzip', 'bz2', 'lzma'\n",
      "            'xz'), the second element must be an integer from 0 to 9, corresponding\n",
      "            to the compression level.\n",
      "        protocol: int, optional\n",
      "            Pickle protocol, see pickle.dump documentation for more details.\n",
      "        cache_size: positive int, optional\n",
      "            This option is deprecated in 0.10 and has no effect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        filenames: list of strings\n",
      "            The list of file names in which the data is stored. If\n",
      "            compress is false, each array is stored in a different file.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        joblib.load : corresponding loader\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Memmapping on load cannot be used for compressed files. Thus\n",
      "        using compression can significantly slow down loading. In\n",
      "        addition, compressed files take extra extra memory during\n",
      "        dump and load.\n",
      "    \n",
      "    effective_n_jobs(n_jobs=-1)\n",
      "        Determine the number of jobs that can actually run in parallel\n",
      "        \n",
      "        n_jobs is the number of workers requested by the callers. Passing n_jobs=-1\n",
      "        means requesting all available workers for instance matching the number of\n",
      "        CPU cores on the worker host(s).\n",
      "        \n",
      "        This method should return a guesstimate of the number of workers that can\n",
      "        actually perform work concurrently with the currently enabled default\n",
      "        backend. The primary use case is to make it possible for the caller to know\n",
      "        in how many chunks to slice the work.\n",
      "        \n",
      "        In general working on larger data chunks is more efficient (less scheduling\n",
      "        overhead and better use of CPU cache prefetching heuristics) as long as all\n",
      "        the workers have enough work to do.\n",
      "        \n",
      "        Warning: this function is experimental and subject to change in a future\n",
      "        version of joblib.\n",
      "        \n",
      "        .. versionadded:: 0.10\n",
      "    \n",
      "    hash(obj, hash_name='md5', coerce_mmap=False)\n",
      "        Quick calculation of a hash to identify uniquely Python objects\n",
      "        containing numpy arrays.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        hash_name: 'md5' or 'sha1'\n",
      "            Hashing algorithm used. sha1 is supposedly safer, but md5 is\n",
      "            faster.\n",
      "        coerce_mmap: boolean\n",
      "            Make no difference between np.memmap and np.ndarray\n",
      "    \n",
      "    load(filename, mmap_mode=None)\n",
      "        Reconstruct a Python object from a file persisted with joblib.dump.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <persistence>`.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        filename: str, pathlib.Path, or file object.\n",
      "            The file object or path of the file from which to load the object\n",
      "        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n",
      "            If not None, the arrays are memory-mapped from the disk. This\n",
      "            mode has no effect for compressed files. Note that in this\n",
      "            case the reconstructed object might no longer match exactly\n",
      "            the originally pickled object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result: any Python object\n",
      "            The object stored in the file.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        joblib.dump : function to save an object\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        This function can load numpy array files saved separately during the\n",
      "        dump. If the mmap_mode argument is given, it is passed to np.load and\n",
      "        arrays are loaded as memmaps. As a consequence, the reconstructed\n",
      "        object might not match the original pickled object. Note that if the\n",
      "        file was saved with compression, the arrays cannot be memmapped.\n",
      "    \n",
      "    register_compressor(compressor_name, compressor, force=False)\n",
      "        Register a new compressor.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        compressor_name: str.\n",
      "            The name of the compressor.\n",
      "        compressor: CompressorWrapper\n",
      "            An instance of a 'CompressorWrapper'.\n",
      "    \n",
      "    register_parallel_backend(name, factory, make_default=False)\n",
      "        Register a new Parallel backend factory.\n",
      "        \n",
      "        The new backend can then be selected by passing its name as the backend\n",
      "        argument to the Parallel class. Moreover, the default backend can be\n",
      "        overwritten globally by setting make_default=True.\n",
      "        \n",
      "        The factory can be any callable that takes no argument and return an\n",
      "        instance of ``ParallelBackendBase``.\n",
      "        \n",
      "        Warning: this function is experimental and subject to change in a future\n",
      "        version of joblib.\n",
      "        \n",
      "        .. versionadded:: 0.10\n",
      "    \n",
      "    register_store_backend(backend_name, backend)\n",
      "        Extend available store backends.\n",
      "        \n",
      "        The Memory, MemorizeResult and MemorizeFunc objects are designed to be\n",
      "        agnostic to the type of store used behind. By default, the local file\n",
      "        system is used but this function gives the possibility to extend joblib's\n",
      "        memory pattern with other types of storage such as cloud storage (S3, GCS,\n",
      "        OpenStack, HadoopFS, etc) or blob DBs.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        backend_name: str\n",
      "            The name identifying the store backend being registered. For example,\n",
      "            'local' is used with FileSystemStoreBackend.\n",
      "        backend: StoreBackendBase subclass\n",
      "            The name of a class that implements the StoreBackendBase interface.\n",
      "    \n",
      "    wrap_non_picklable_objects(obj, keep_wrapper=True)\n",
      "        Wrapper for non-picklable object to use cloudpickle to serialize them.\n",
      "        \n",
      "        Note that this wrapper tends to slow down the serialization process as it\n",
      "        is done with cloudpickle which is typically slower compared to pickle. The\n",
      "        proper way to solve serialization issues is to avoid defining functions and\n",
      "        objects in the main scripts and to implement __reduce__ functions for\n",
      "        complex classes.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Memory', 'MemorizedResult', 'PrintTime', 'Logger', 'hash',...\n",
      "\n",
      "VERSION\n",
      "    0.13.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\wang\\anaconda3\\lib\\site-packages\\joblib\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. \n",
    "# Please import this functionality directly from joblib, which can be installed with: \n",
    "# pip install joblib.\n",
    "import joblib\n",
    "help(joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
